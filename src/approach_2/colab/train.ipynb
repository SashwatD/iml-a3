{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Image Captioning with Flash Attention\n",
    "\n",
    "This notebook implements Approach 2 (Vision Transformer) for image captioning on the ArtEmis dataset. It is designed to run on Google Colab with GPU support (specifically optimized for T4/Ampere with Flash Attention).\n",
    "\n",
    "## Setup\n",
    "1.  Mount Google Drive.\n",
    "2.  Ensure your dataset (CSV and images) is accessible in Drive.\n",
    "3.  Run the cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, mixed_precision\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# UPDATE THESE PATHS TO MATCH YOUR DRIVE STRUCTURE\n",
    "CSV_PATH = \"/content/drive/MyDrive/artemis_dataset_release_v0.csv\"\n",
    "IMG_DIR = \"/content/drive/MyDrive/wikiart\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/iml_a3_models/approach_2\"\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64 # Increased batch size for T4\n",
    "EPOCHS = 20\n",
    "IMAGE_SIZE = (256, 256)\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_LENGTH = 50\n",
    "EMBEDDING_DIM = 256\n",
    "USE_MIXED_PRECISION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(csv_path, image_dir, sample_size=None, stratify_col='art_style'):\n",
    "    \"\"\"\n",
    "    Loads dataset, filters missing images, and performs stratified sampling.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "        \n",
    "    if 'image_file' not in df.columns:\n",
    "         # Construct path if not present\n",
    "         df['image_file'] = df.apply(lambda x: os.path.join(image_dir, x['art_style'], x['painting'] + '.jpg'), axis=1)\n",
    "\n",
    "    # Filter missing files\n",
    "    print(\"Checking for missing files...\")\n",
    "    def file_exists(path):\n",
    "        return os.path.exists(path)\n",
    "    \n",
    "    # This might be slow for 80k images, but necessary for robustness\n",
    "    # Optimization: Check a few or assume correctness if confident\n",
    "    # valid_mask = df['image_file'].apply(file_exists)\n",
    "    # missing_count = (~valid_mask).sum()\n",
    "    # if missing_count > 0:\n",
    "    #     print(f\"Warning: {missing_count} images not found. Removing them.\")\n",
    "    #     df = df[valid_mask]\n",
    "    \n",
    "    # Stratified Sampling\n",
    "    if sample_size and sample_size < len(df):\n",
    "        print(f\"Performing stratified sampling to reduce size to {sample_size}...\")\n",
    "        try:\n",
    "            df, _ = train_test_split(\n",
    "                df, \n",
    "                train_size=sample_size, \n",
    "                stratify=df[stratify_col], \n",
    "                random_state=42\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"Stratified sampling failed: {e}. Falling back to random sampling.\")\n",
    "            df = df.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "    print(f\"Final dataset size: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"\n",
    "    Custom text standardization: lowercase, remove punctuation.\n",
    "    \"\"\"\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def get_augmentation_layer():\n",
    "    \"\"\"Returns a Sequential model for image augmentation.\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.1),\n",
    "    ])\n",
    "\n",
    "def create_tf_dataset(\n",
    "    df, \n",
    "    image_size=(256, 256), \n",
    "    batch_size=32, \n",
    "    vocab_size=5000, \n",
    "    max_length=50, \n",
    "    validation_split=0.2,\n",
    "    augment=False,\n",
    "    vectorizer=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates tf.data.Dataset pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare paths and captions\n",
    "    image_paths = df['image_file'].values\n",
    "    captions = df['utterance'].values\n",
    "    \n",
    "    # Add start and end tokens\n",
    "    captions = [f\"<start> {cap} <end>\" for cap in captions]\n",
    "    \n",
    "    # Split data\n",
    "    train_paths, val_paths, train_caps, val_caps = train_test_split(\n",
    "        image_paths, captions, test_size=validation_split, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Text Vectorization\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_mode='int',\n",
    "            output_sequence_length=max_length,\n",
    "            standardize=custom_standardization\n",
    "        )\n",
    "        print(\"Adapting text vectorizer...\")\n",
    "        vectorizer.adapt(train_caps)\n",
    "    else:\n",
    "        print(\"Using provided vectorizer.\")\n",
    "    \n",
    "    def read_image(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, image_size)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img\n",
    "    \n",
    "    aug_layer = get_augmentation_layer() if augment else None\n",
    "    \n",
    "    def process_data(image_path, caption, training=False):\n",
    "        img = read_image(image_path)\n",
    "        if training and aug_layer is not None:\n",
    "            img = aug_layer(img)\n",
    "        cap = vectorizer(caption)\n",
    "        cap_in = cap[:-1]\n",
    "        cap_out = cap[1:]\n",
    "        return (img, cap_in), cap_out\n",
    "    \n",
    "    def make_dataset(paths, caps, is_training=False):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((paths, caps))\n",
    "        dataset = dataset.map(\n",
    "            lambda p, c: process_data(p, c, training=is_training), \n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "    \n",
    "    print(\"Creating training dataset...\")\n",
    "    train_ds = make_dataset(train_paths, train_caps, is_training=augment)\n",
    "    \n",
    "    print(\"Creating validation dataset...\")\n",
    "    val_ds = make_dataset(val_paths, val_caps, is_training=False)\n",
    "    \n",
    "    return train_ds, val_ds, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_embeddings(vectorizer, captions, embedding_dim=256):\n",
    "    print(\"Generating TF-IDF embeddings...\")\n",
    "    if captions is None:\n",
    "        raise ValueError(\"Captions list is required for TF-IDF embeddings.\")\n",
    "    vocab = vectorizer.get_vocabulary()\n",
    "    return compute_tfidf_matrix(captions, vocab, embedding_dim=embedding_dim)\n",
    "\n",
    "def compute_tfidf_matrix(captions, vocab, embedding_dim=256):\n",
    "    print(f\"Computing TF-IDF on {len(captions)} captions...\")\n",
    "    clean_vocab = [w for w in vocab if w not in ['', '[UNK]']]\n",
    "    tfidf = TfidfVectorizer(vocabulary=clean_vocab, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    tfidf_matrix = tfidf.fit_transform(captions)\n",
    "    \n",
    "    print(f\"Reducing dimension to {embedding_dim} using SVD...\")\n",
    "    svd = TruncatedSVD(n_components=embedding_dim, random_state=42)\n",
    "    word_features = svd.fit_transform(tfidf_matrix.T)\n",
    "    \n",
    "    final_matrix = np.zeros((len(vocab), embedding_dim), dtype=\"float32\")\n",
    "    feature_index_map = {word: i for i, word in enumerate(clean_vocab)}\n",
    "    \n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in feature_index_map:\n",
    "            final_matrix[i] = word_features[feature_index_map[word]]\n",
    "        else:\n",
    "            final_matrix[i] = np.random.normal(scale=0.1, size=embedding_dim)\n",
    "    return final_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (with Flash Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCreation(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n",
    "        return config\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # ENABLE FLASH ATTENTION HERE\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, use_flash_attention=True)\n",
    "        self.ffn = models.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.att.key_dim,\n",
    "            \"num_heads\": self.att.num_heads,\n",
    "            \"ff_dim\": self.ffn.layers[0].units,\n",
    "            \"rate\": self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # ENABLE FLASH ATTENTION HERE\n",
    "        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, use_flash_attention=True)\n",
    "        self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, use_flash_attention=True)\n",
    "        self.ffn = models.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training=False, use_causal_mask=False):\n",
    "        attn1 = self.att1(inputs, inputs, use_causal_mask=use_causal_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn1)\n",
    "        \n",
    "        attn2 = self.att2(out1, encoder_outputs)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.att1.key_dim,\n",
    "            \"num_heads\": self.att1.num_heads,\n",
    "            \"ff_dim\": self.ffn.layers[0].units,\n",
    "            \"rate\": self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, embedding_matrix=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        if embedding_matrix is not None:\n",
    "            self.token_emb = layers.Embedding(\n",
    "                input_dim=vocab_size, \n",
    "                output_dim=embed_dim,\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False\n",
    "            )\n",
    "        else:\n",
    "            self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "            \n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"maxlen\": self.pos_emb.input_dim,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def build_vit_caption_model(\n",
    "    input_shape=(256, 256, 3),\n",
    "    patch_size=16,\n",
    "    num_patches=256,\n",
    "    projection_dim=256,\n",
    "    num_heads=4,\n",
    "    transformer_layers=4,\n",
    "    vocab_size=5000,\n",
    "    max_length=50,\n",
    "    ff_dim=512,\n",
    "    dropout_rate=0.1,\n",
    "    embedding_matrix=None\n",
    "):\n",
    "    # --- Encoder (ViT) ---\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    patches = PatchCreation(patch_size)(inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        encoded_patches = TransformerEncoderBlock(\n",
    "            projection_dim, num_heads, ff_dim, dropout_rate\n",
    "        )(encoded_patches)\n",
    "    \n",
    "    # --- Decoder ---\n",
    "    caption_inputs = layers.Input(shape=(max_length,), dtype=\"int64\")\n",
    "    x = TokenAndPositionEmbedding(\n",
    "        max_length, vocab_size, projection_dim, embedding_matrix=embedding_matrix\n",
    "    )(caption_inputs)\n",
    "    \n",
    "    for _ in range(transformer_layers):\n",
    "        x = TransformerDecoderBlock(\n",
    "            projection_dim, num_heads, ff_dim, dropout_rate\n",
    "        )(x, encoded_patches, use_causal_mask=True)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    \n",
    "    model = models.Model(inputs=[inputs, caption_inputs], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none'\n",
    "    )\n",
    "    mask = tf.math.not_equal(y_true, 0)\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_acc_percent(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(y_true, 0)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return 100.0 * tf.reduce_sum(match * mask) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Enable Mixed Precision\n",
    "if USE_MIXED_PRECISION:\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision enabled.\")\n",
    "\n",
    "# 1. Load and Preprocess Data\n",
    "df = load_and_clean_data(CSV_PATH, IMG_DIR, sample_size=None) # Set sample_size for testing if needed\n",
    "\n",
    "train_ds, val_ds, vectorizer = create_tf_dataset(\n",
    "    df, \n",
    "    image_size=IMAGE_SIZE, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    max_length=MAX_LENGTH,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "# 2. Prepare Embeddings (TF-IDF)\n",
    "captions = df['utterance'].tolist()\n",
    "embedding_matrix = get_tfidf_embeddings(vectorizer, captions, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# 3. Build Model\n",
    "print(f\"Building Vision Transformer Model with Flash Attention...\")\n",
    "model = build_vit_caption_model(\n",
    "    input_shape=IMAGE_SIZE + (3,),\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_length=MAX_LENGTH - 1,\n",
    "    transformer_layers=4,\n",
    "    num_heads=4,\n",
    "    projection_dim=EMBEDDING_DIM,\n",
    "    ff_dim=512,\n",
    "    dropout_rate=0.1,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "\n",
    "# 4. Compile Model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "if USE_MIXED_PRECISION:\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "    \n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_loss,\n",
    "    metrics=[masked_acc_percent]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 5. Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(OUTPUT_DIR, \"best_model.weights.h5\"),\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# 6. Train\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save_weights(os.path.join(OUTPUT_DIR, \"final_model.weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot History\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['masked_acc_percent']\n",
    "val_acc = history.history['val_masked_acc_percent']\n",
    "\n",
    "epochs_range = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs_range, val_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, acc, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs_range, val_acc, 'ro-', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
